# Pnew = exp(cbind(1,Xnew) %*% beta[[1]])/(1+exp(cbind(1,Xnew) %*% beta[[1]]))
# ynew = rbinom(Nnew,size=1,prob = Pnew)
Xref = MASS::mvrnorm(n = Nref, rep(0,p), Sx) #mvrnorm(Nref,mu=rep(0,K),Sigma = Sx)
Xref1 = cbind(1, Xref)
colnames(Xref) = paste0('x',1:p)
N = X = xk = xkref = xk1refs = y = P = list()
thetak = sk = suk = list()
### individual data by study:
# training sample
xkref1 = list()
for (k in 1:K){
N[[k]] = Ns[k] # settings[set, 'Ns']
X[[k]] = mvrnorm(N[[k]],mu=rep(0,p),Sigma = Sx)
xk[[k]] = X[[k]][,ind[[k]]]
xkref[[k]] = Xref[,ind[[k]]]
xkref1[[k]] = cbind(1,xkref[[k]])
xk1refs[[k]] = lapply(1:Nref, function(x){c(1,xkref[[k]][x,])})
P[[k]] = exp(cbind(1,X[[k]]) %*% beta[[k]])/(1+exp(cbind(1,X[[k]]) %*% beta[[k]]))
y[[k]] = rbinom(N[[k]],size=1,prob = P[[k]])
#y[[k]] = X[[k]] %*% (beta[[k]]) + rnorm(N[[k]],mean=rep(0,K),sd = sqrt(vx))
dat = data.frame(y=y[[k]],x=xk[[k]])
colnames(dat) = c('y',paste0('x',ind[[k]]))
fo = as.formula(paste('y', paste(paste0('x',ind[[k]]), collapse=" + "), sep=" ~ "))
fit = glm(fo,data=dat, family = binomial(link = "logit"))
thetak[[k]] = summary(fit)$coef[c('(Intercept)',paste0('x',ind[[k]])),'Estimate']; #sk[[k]] = summary(fit)$coef[paste0('x',1:length(ind[[k]])),'Std. Error']
names(thetak[[k]]) = c('(Intercept)',paste0('x',ind[[k]]))
suk[[k]] = vcov(fit) #summary(fit)$coef[,'Std. Error']
#thetak[[k]][1] = 0
}
re = list()
if (wm == 1){
if (p<8){
st0 = 8e-4; st = rep(st0,p) # 3e-5
}
if ((p>=8)&(p<16)){
st0 = 3e-4; st = rep(st0,p) # 1e-5
}
if ((p>=16)){
st0 = 1e-4; st = rep(st0,p) # 3e-6
}
}
if (wm == 0){
if (p<8){
st0 = 3e-5; st = rep(st0,p) # 3e-5
}
if ((p>=8)&(p<16)){
st0 = 1e-5; st = rep(st0,p) # 1e-5
}
if ((p>=16)){
st0 = 3e-6; st = rep(st0,p) # 3e-6
}
}
theta.gmm = studies = list()
for (k in 1:K){
dat = data.frame(y=y[[k]],x=xk[[k]])
colnames(dat) = c('y',paste0('x',ind[[k]]))
fo = as.formula(paste('y', paste(paste0('x',ind[[k]]), collapse=" + "), sep=" ~ "))
fit = glm(fo,data=dat, family = "binomial")
theta.gmm[[k]] = fit$coefficients
names(theta.gmm[[k]])[-1] = c(paste0('x',ind[[k]]))
studies[[k]] = list(Coeff = theta.gmm[[k]], Covariance = NULL, Sample_size = N[[k]])
}
model = 'logistic'
reference = cbind(1,Xref)
colnames(reference)[1] = '(Intercept)'
set.seed(sim*2022)
initvalue = beta0 + runif(n=K,-0.2,0.2); names(initvalue) = paste0('x',1:K)
library(MASS)
library(GENMETA)
temp <- commandArgs(TRUE)
wm = 1
sim = 1
set = 1
n.chain = 3
cal_uk = function(xkref,thetak,Xref,beta){
e_x_beta = exp(Xref%*%beta) # intercept included
e_xk_betak = exp(xkref %*% (thetak))
t(xkref) %*% ((e_x_beta - e_xk_betak) * (1/((1+e_xk_betak)*(1+e_x_beta))) )
}
# wm = 1 # use optimal weighting matrix?
nsim = 100 #####
Nnew = 5000
threshold = 1e-6 # for myoptim
model = 'logistic'
ind = list(c(1,2), c(1,2), c(1,3), c(1,3), c(1,4), c(1,4),
c(2,3), c(2,3), c(2,4), c(2,4), c(3,4), c(3,4))
ps = c(4); p = ps
Ks = c(2) # 1 #c(2,4) # how many times does each parameter show up
K = length(ind) #Ks * p #settings[set, 'K'] * p
Nrefs = round(seq_log(50, 1000, length.out=10)) #seq(50,1200,by=200)
Ns = seq(500,500*K,by=500) # Sample Size
settings = expand.grid(Nref = Nrefs)
#for (set in 1:nrow(settings)){
Nref = settings[set, 'Nref']
#p = settings[set, 'p']
if (wm == 0){
if (p<8){
nit = 2e4; n.burnin = round(nit/2)
}
if ((p>=8)&(p<16)){
nit = 5e4; n.burnin = round(nit/2)
}
if ((p>=16)){
nit = 2e5; n.burnin = round(nit/2)
}
}
if (wm == 1){
if (p<8){
nit = 2e4; n.burnin = round(nit/2)
}
if ((p>=8)&(p<16)){
nit = 5e4; n.burnin = round(nit/2)
}
if ((p>=16)){
nit = 2e5; n.burnin = round(nit/2)
}
}
Sx = matrix(0.3,p,p); diag(Sx) = 1
#vx = 2
#Sbeta = diag(1,)%*%matrix(c(1,0.6,0.5,0.6,1,0.3,0.5,0.3,1),3,3)%*%diag(c(1,2,3))
var.re = 0.1
Sbeta = matrix(0.3,p,p); diag(Sbeta) = 1; Sbeta = Sbeta * var.re
res = matrix(0,nsim,6)
colnames(res) = c('bias.bayes','rmse.bayes','roc.bayes','bias.gmm','rmse.gmm','roc.gmm')
#for (sim in 1:nsim){
set.seed(sim * 2022)
signs = c(rep(-1,p/2), rep(1, p/2))
beta0 = rep(0.5) * signs
beta = list()
output = list()
alpha=-1.5
int0 = alpha
intk0 = list()
set.seed(sim * 2021)
signs = c(rep(1,K/2), rep(-1, K/2))
for (k in 1:K){
intk0[[k]] = alpha #+ 0.2 * signs[k] #rnorm(n=1,alpha,0.2)
#beta[[k]] = c(intk0, mvrnorm(1,mu = beta0,Sigma=Sbeta))
beta[[k]] = c(intk0[[k]], beta0) #c(intk0[[k]], beta0)
}
m = ceiling(p/2) # variables per study
set.seed(sim*2022)
# Xnew = mvrnorm(Nnew,mu=rep(0,p),Sigma = Sx)
# Pnew = exp(cbind(1,Xnew) %*% beta[[1]])/(1+exp(cbind(1,Xnew) %*% beta[[1]]))
# ynew = rbinom(Nnew,size=1,prob = Pnew)
Xref = MASS::mvrnorm(n = Nref, rep(0,p), Sx) #mvrnorm(Nref,mu=rep(0,K),Sigma = Sx)
Xref1 = cbind(1, Xref)
colnames(Xref) = paste0('x',1:p)
N = X = xk = xkref = xk1refs = y = P = list()
thetak = sk = suk = list()
### individual data by study:
# training sample
xkref1 = list()
for (k in 1:K){
N[[k]] = Ns[k] # settings[set, 'Ns']
X[[k]] = mvrnorm(N[[k]],mu=rep(0,p),Sigma = Sx)
xk[[k]] = X[[k]][,ind[[k]]]
xkref[[k]] = Xref[,ind[[k]]]
xkref1[[k]] = cbind(1,xkref[[k]])
xk1refs[[k]] = lapply(1:Nref, function(x){c(1,xkref[[k]][x,])})
P[[k]] = exp(cbind(1,X[[k]]) %*% beta[[k]])/(1+exp(cbind(1,X[[k]]) %*% beta[[k]]))
y[[k]] = rbinom(N[[k]],size=1,prob = P[[k]])
#y[[k]] = X[[k]] %*% (beta[[k]]) + rnorm(N[[k]],mean=rep(0,K),sd = sqrt(vx))
dat = data.frame(y=y[[k]],x=xk[[k]])
colnames(dat) = c('y',paste0('x',ind[[k]]))
fo = as.formula(paste('y', paste(paste0('x',ind[[k]]), collapse=" + "), sep=" ~ "))
fit = glm(fo,data=dat, family = binomial(link = "logit"))
thetak[[k]] = summary(fit)$coef[c('(Intercept)',paste0('x',ind[[k]])),'Estimate']; #sk[[k]] = summary(fit)$coef[paste0('x',1:length(ind[[k]])),'Std. Error']
names(thetak[[k]]) = c('(Intercept)',paste0('x',ind[[k]]))
suk[[k]] = vcov(fit) #summary(fit)$coef[,'Std. Error']
#thetak[[k]][1] = 0
}
re = list()
if (wm == 1){
if (p<8){
st0 = 8e-4; st = rep(st0,p) # 3e-5
}
if ((p>=8)&(p<16)){
st0 = 3e-4; st = rep(st0,p) # 1e-5
}
if ((p>=16)){
st0 = 1e-4; st = rep(st0,p) # 3e-6
}
}
if (wm == 0){
if (p<8){
st0 = 3e-5; st = rep(st0,p) # 3e-5
}
if ((p>=8)&(p<16)){
st0 = 1e-5; st = rep(st0,p) # 1e-5
}
if ((p>=16)){
st0 = 3e-6; st = rep(st0,p) # 3e-6
}
}
genmeta_start_time <- Sys.time()
theta.gmm = studies = list()
for (k in 1:K){
dat = data.frame(y=y[[k]],x=xk[[k]])
colnames(dat) = c('y',paste0('x',ind[[k]]))
fo = as.formula(paste('y', paste(paste0('x',ind[[k]]), collapse=" + "), sep=" ~ "))
fit = glm(fo,data=dat, family = "binomial")
theta.gmm[[k]] = fit$coefficients
names(theta.gmm[[k]])[-1] = c(paste0('x',ind[[k]]))
studies[[k]] = list(Coeff = theta.gmm[[k]], Covariance = NULL, Sample_size = N[[k]])
}
model = 'logistic'
reference = cbind(1,Xref)
colnames(reference)[1] = '(Intercept)'
set.seed(sim*2022)
initvalue = beta0 + runif(n=K,-0.2,0.2); names(initvalue) = paste0('x',1:K)
a = GENMETA(studies, reference, model, variable_intercepts=T)
a$Est.coeff
a = GENMETA(studies, reference, model, variable_intercepts=F)
a$Est.coeff
require(stats)
require(graphics)
library(splines2)
library(coneproj)
library(rcdd)
library(truncnorm)
library(truncdist)
library(mvtnorm)
library(R.utils)
library(Matrix)
library(igraph)
library(MCMCpack)
library(fda)
library(nnls)
source("/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava/fittingCompoSpikeFinal - Mixed effect.R")
source('/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava/fittingCompo - Mixed effect.R')
n <- 100
Ti <- 20
Tiv <- 1:20
sd = 0.5;m=0
x = seq(-2,2,length.out = Ti)
x = sort(x)
set.seed(11131991)
fx = dnorm(x,mean=m,sd=sd) ### true mean
plot(x,fx)
ftf <- 3*fx
## finding the A matrix for bell-shaped
d2fx = function(x){
return(dnorm(x,mean=m,sd=sd)*(-1+(x-m)^2/sd^2)/sd^2)
}
d2fx(x)
ind = which(d2fx(x)<0)
t=x
i1 = ind[1]
i2 = ind[length(ind)]
# 20 30
A = matrix(0,nrow=Ti+2,ncol=Ti)
for(j in 1:(i1-2)){
A[j,j] = t[(j+2)]-t[(j+1)]
A[j,(j+1)] = t[j]-t[(j+2)]
A[j,(j+2)] = t[j+1]-t[j]
}
for(j in (i2):(Ti-2)){
A[j,j] = t[(j+2)]-t[(j+1)]
A[j,(j+1)] = t[j]-t[(j+2)]
A[j,(j+2)] = t[j+1]-t[j]
}
for(j in (i1-1):(i2-1)){
A[j,j] = t[(j+1)]-t[(j+2)]
A[j,(j+1)] = -t[j]+t[(j+2)]
A[j,(j+2)] = -t[j+1]+t[j]
}
## added by me
#A[i2-1,(i2-2):i2] = - A[i2-1,(i2-2):i2]
A[(Ti-1),1] = -1.0
A[(Ti-1),2] = 1.0
A[Ti,1] = 1.0
A[(Ti+1),(Ti-1)] = 1.0
A[(Ti+1),Ti] = -1.0
A[(Ti+2),Ti] = 1.0
m = nrow(A)
### makeH uses Ax <= 0; so multiply by -1
B = -A
qux <- makeH(B, rep(0, m))
### makeH produces output A such that Ax >= 0
#print(qux)
out <- scdd(d2q(qux),adjacency = T,incidence = T,inputincidence = T,inputadjacency = T, representation = "H")
out1 = q2d(out$output)
#Why removing first two columns? Is it specific to this simulation setting? Or should it be for any A?
Delta = t(out1[,-c(1:2)])
dim(Delta)
adjacency = out$adjacency
adj.list = graph_from_adj_list(adjacency, mode = c("out", "in", "all", "total"),duplicate = TRUE)
adj.mat = as_adjacency_matrix(adj.list, sparse=F)
### cliques
graph.list = graph_from_adjacency_matrix(adj.mat, "undirected", weighted=T, diag=F)
clique.list = max_cliques(graph.list)
clique.size = unlist(lapply(clique.list,length))
degree = clique.size
l = count_max_cliques(graph.list)
b.ind = as.vector(clique.list[[2]])
ftf = rowMeans(Delta[,b.ind])
plot(x,ftf,type="b",ylab="f(x)")
###############model fitting
J    <- 20
knot <- J-3
BS1 <- bsplineS((1:Ti)/Ti, breaks = seq(0, 1, 1/knot))
#A <- A %*% BS1
A <- A %*% BS1
m = nrow(A)
### makeH uses Ax <= 0; so multiply by -1
B = -A
qux <- makeH(B, rep(0, m))
### makeH produces output A such that Ax >= 0
#print(qux)
out <- scdd(d2q(qux),adjacency = T,incidence = T,inputincidence = T,inputadjacency = T, representation = "H")
out1 = q2d(out$output)
#Why removing first two columns? Is it specific to this simulation setting? Or should it be for any A?
Delta = t(out1[,-c(1:2)])
dim(Delta)
adjacency = out$adjacency
adj.list = graph_from_adj_list(adjacency, mode = c("out", "in", "all", "total"),duplicate = TRUE)
adj.mat = as_adjacency_matrix(adj.list, sparse=F)
### cliques
graph.list = graph_from_adjacency_matrix(adj.mat, "undirected", weighted=T, diag=F)
clique.list = max_cliques(graph.list)
clique.size = unlist(lapply(clique.list,length))
degree = clique.size
l = count_max_cliques(graph.list)
gamma_ls <- list()
rep0 = 50
err <- array(0, c(rep0, 2, 3))
esti1 <- array(0, c(rep0, Ti, 3))
esti2 <- array(0, c(rep0, Ti, 3))
sigl <- c(0.5, 1, 2)
for(j in 1:3){
print(j)
for(rep in 1:rep0)
{
print(rep)
#Generate data
#j=1;rep=1
xt <- rep(1:Ti, n)
id <- 1
while(length(table(id)) < n){
ind <- sample(1:(Ti*n), 0.2*Ti*n)
id <- rep(1:n, each=Ti)
id <- id[-ind]
}
xt <- xt[-ind]
fxt <- ftf[xt]#exp(-abs(xt-2.6)/3) + a^(-abs(xt-2.6)/3)
#fxt[which(xt <=10 & xt>=7)] <- ft[7]
#plot(fxt)
Sigma <- exp(-as.matrix(dist(1:Ti))/4)
W <- mvtnorm::rmvnorm(n, sigma = Sigma)
W <- array(t(W))[-ind]
sige <- sigl[j]
e <- rnorm(length(xt), sd=sige)
y <- array(fxt + W + e)
#A <- rbind(A, diag(J))
#BS <- matrix(0, length(y), Ti)
#BS[cbind(1:length(y), xt)] <- 1#bsplineS(xt/Ti, breaks = seq(0, 1, 1/knot))
BS <- bsplineS(xt/Ti, breaks = seq(0, 1, 1/knot))
Deltatilde <- BS %*% Delta
Als <- list()
for(i in 1:n){
ni <- which(id==i)
Als[[i]] <- bsplineS(xt[ni]/Ti, breaks = seq(0, 1, 1/knot))
}
X <- Deltatilde
cliques=clique.list
Total_itr=30
out2 <- fittingUN(y, xt, Ti, Total_itr=30)
out1 <- fitting(y, X, BS1, cliques=clique.list, Total_itr=30)
err[rep, 1, j] <- out1$err
err[rep, 2, j] <- out2$err
esti1[rep, , j] <- out1$esti
esti2[rep, , j] <- out2$esti
gamma_ls[[rep+(j-1)*rep0]] <- out1$gamma
#print(err[rep, ])
}
}
### 1st row spline + restricted
### 2nd row spline + unrestricted
apply(err, 2:3, median)
apply(err, 2:3, mean)
check = A%*%ftf
check
setwd("~/Desktop/Backup/priors_on_cone/projects/arkaprava/strong_sparse")
apply(err, 2:3, median)
apply(err, 2:3, mean)
### 1st row spline + restricted
### 2nd row spline + unrestricted
err.median = apply(err, 2:3, median)
err.mean = apply(err, 2:3, mean)
err = round(rbind(err.median,err.mean),3)
colnames(err) = c("sd=0.5","sd=1","sd=2")
err
rownames(err) = c("median RS","median URS","mean RS","mean URS")
err
setwd("~/Desktop/Backup/priors_on_cone/projects/arkaprava/strong_sparse")
write.csv(err,"Bellshaped_spline_clique.csv")
### esti1 columns are each time point; rows are reps; j=3 matrices for 3 diff sd values
estim <- apply(esti1, 2:3, median)
estim2 <- apply(esti2, 2:3, median)
estim3 <- apply(esti3, 2:3, median)
library(reshape)
library(ggplot2)
dat.true = data.frame(x=1:Ti,y = ftf)
dat.sd1 = data.frame(x=1:Ti,RS=estim[,1],URS=estim2[,1])
dat.sd1 = melt(dat.sd1,id.vars = "x")
dat.sd1$sd = "sd=0.5"
dat.sd2 = data.frame(x=1:Ti,RS=estim[,2],URS=estim2[,2])
dat.sd2 = melt(dat.sd2,id.vars = "x")
dat.sd2$sd = "sd=1"
dat.sd3 = data.frame(x=1:Ti,RS=estim[,3],URS=estim2[,3])
dat.sd3 = melt(dat.sd3,id.vars = "x")
dat.sd3$sd = "sd=2"
dat = rbind(dat.sd1,dat.sd2,dat.sd3)
p1 = ggplot()+
geom_line(data = dat,aes(x=x,y=value,color=variable))+facet_wrap(~sd)+
#  scale_linetype_manual(values=c("dashed","solid","solid","solid"))+
theme(text = element_text(size = 35))+
geom_point(data = dat.true,aes(x=x,y=y))
p1
#theme_classic()+
#theme(legend.position=c(0.88,0.15),legend.key.size = unit(1.5, 'cm'),
#      legend.title = element_blank())
ggsave('Bellshaped_spline_clique.png', p1, width = 17,height=11, dpi = 300)
require(stats)
require(graphics)
library(splines2)
library(coneproj)
library(rcdd)
library(truncnorm)
library(truncdist)
library(mvtnorm)
library(R.utils)
library(Matrix)
library(igraph)
library(MCMCpack)
library(fda)
library(nnls)
source("/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava/fittingCompoSpikeFinal - Mixed effect.R")
source('/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava/fittingCompo - Mixed effect.R')
n <- 100
Ti <- 20
Tiv <- 1:20
sd = 0.5;m=0
x = seq(-2,2,length.out = Ti)
x = sort(x)
set.seed(11131991)
fx = dnorm(x,mean=m,sd=sd) ### true mean
plot(x,fx)
ftf <- 3*fx
## finding the A matrix for bell-shaped
d2fx = function(x){
return(dnorm(x,mean=m,sd=sd)*(-1+(x-m)^2/sd^2)/sd^2)
}
d2fx(x)
ind = which(d2fx(x)<0)
t=x
i1 = ind[1]
i2 = ind[length(ind)]
# 20 30
A = matrix(0,nrow=Ti+2,ncol=Ti)
for(j in 1:(i1-2)){
A[j,j] = t[(j+2)]-t[(j+1)]
A[j,(j+1)] = t[j]-t[(j+2)]
A[j,(j+2)] = t[j+1]-t[j]
}
for(j in (i2):(Ti-2)){
A[j,j] = t[(j+2)]-t[(j+1)]
A[j,(j+1)] = t[j]-t[(j+2)]
A[j,(j+2)] = t[j+1]-t[j]
}
for(j in (i1-1):(i2-1)){
A[j,j] = t[(j+1)]-t[(j+2)]
A[j,(j+1)] = -t[j]+t[(j+2)]
A[j,(j+2)] = -t[j+1]+t[j]
}
## added by me
#A[i2-1,(i2-2):i2] = - A[i2-1,(i2-2):i2]
A[(Ti-1),1] = -1.0
A[(Ti-1),2] = 1.0
A[Ti,1] = 1.0
A[(Ti+1),(Ti-1)] = 1.0
A[(Ti+1),Ti] = -1.0
A[(Ti+2),Ti] = 1.0
###############model fitting
m = nrow(A)
### makeH uses Ax <= 0; so multiply by -1
B = -A
qux <- makeH(B, rep(0, m))
### makeH produces output A such that Ax >= 0
#print(qux)
out <- scdd(d2q(qux),adjacency = T,incidence = T,inputincidence = T,inputadjacency = T, representation = "H")
out1 = q2d(out$output)
#Why removing first two columns? Is it specific to this simulation setting? Or should it be for any A?
Delta = t(out1[,-c(1:2)])
dim(Delta)
adjacency = out$adjacency
adj.list = graph_from_adj_list(adjacency, mode = c("out", "in", "all", "total"),duplicate = TRUE)
adj.mat = as_adjacency_matrix(adj.list, sparse=F)
### cliques
graph.list = graph_from_adjacency_matrix(adj.mat, "undirected", weighted=T, diag=F)
clique.list = max_cliques(graph.list)
clique.size = unlist(lapply(clique.list,length))
degree = clique.size
l = count_max_cliques(graph.list)
b.ind = as.vector(clique.list[[2]])
ftf = rowMeans(Delta[,b.ind])
plot(x,ftf,type="b",ylab="f(x)")
check = A%>%ftf
check = A%*%ftf
check
clique.list[[2]]
