for(j in (i3):(Ti-2)){
A[j+2,j] = t[(j+2)]-t[(j+1)]
A[j+2,(j+1)] = t[j]-t[(j+2)]
A[j+2,(j+2)] = t[j+1]-t[j]
}
A[(Ti+1),(Ti-1)] = 1.0
A[(Ti+1),Ti] = -1.0
A[(Ti+2),Ti] = 1.0
library(rnhanesdata)
library(ggplot2)
## load the data
data("PAXINTEN_C");data("PAXINTEN_D")
data("Flags_C");data("Flags_D")
#data("Mortality_2011_C");data("Mortality_2011_D")
data("Covariate_C");data("Covariate_D")
data("Mortality_2015_C");data("Mortality_2015_D")
## 60*24=1440 minute level data to 24 hour data
timevec <- function(x){
mat <- matrix(x, 60, 24)
return(array(colMeans(mat, na.rm = T)))
}
y <- as.matrix(PAXINTEN_C[, 6:ncol(PAXINTEN_C)])
ydata <- apply(y, 1, timevec)
ydata <- t(ydata)
### 7176 participants for 7 days = 50232
subfreq <- array(table(PAXINTEN_C$SEQN))
sub  <- unique(PAXINTEN_C$SEQN)
### covariates for defining inclusion criteria
indr <- which(Covariate_C$SEQN %in% PAXINTEN_C$SEQN)
Covariate_Cr <- Covariate_C[indr, ]
#Inclusion criteria
ind1 <- which(as.numeric(Covariate_Cr$RIDAGEYR) < 40 &
as.numeric(Covariate_Cr$RIDAGEYR) > 30 &
Covariate_Cr$EducationAdult == "College graduate or above"
)
length(ind1)
samsub <- Covariate_Cr$SEQN[ind1]
### back to activity data
ind <- which(PAXINTEN_C$SEQN %in% samsub)
ydatar <- ydata[ind, ]
Sind1 <- (0:(nrow(ydatar)/7-1))*7+1
Sind2 <- (0:(nrow(ydatar)/7-1))*7+2
Sind3 <- (0:(nrow(ydatar)/7-1))*7+3
Sind4 <- (0:(nrow(ydatar)/7-1))*7+4
Sind5 <- (0:(nrow(ydatar)/7-1))*7+5
Sind6 <- (0:(nrow(ydatar)/7-1))*7+6
Sind7 <- (0:(nrow(ydatar)/7-1))*7+7
ydata1 <- ydatar[Sind1, ]
ind1 <- which(is.na(rowMeans(ydata1)==T))
if(length(ind1)) {ydata1 <- ydata1[-ind1, ]}
day1 = apply(ydata1,2,median)
day1.mean = colMeans(ydata1)
plot(day1.mean,type="l")
plot(day1,type="l")
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(ggpubr)
ydata1 = as.data.frame(t(ydata1))
ydata.plot = ydata1[,1:20]
ydata.plot$x = 1:24
dat = melt(ydata.plot,id.vars = "x")
dat.mean = data.frame(x=1:24,y=day1.mean)
p1 = ggplot()+
geom_line(data = dat,aes(x=x,y=value,color=variable),lwd=0.8)+
geom_line(data = dat.mean,aes(x=x,y=y),size=1.5,linetype="dashed")+
theme_classic()+
theme(legend.position="None",legend.key.size = unit(1.5, 'cm'),
legend.title = element_blank())+
theme(legend.background = element_rect(colour = 'black', fill = 'white', linetype='solid'))+
theme(text = element_text(size = 30))+
labs(x ="Time (in hours)", y = "Hourly activity Count")+
scale_color_manual(values = c(brewer.pal(10, name="Spectral"),brewer.pal(10,  "Paired")))+
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_rect(colour = "black", size=0.9))
p1
ggsave('20_subjects.png', p1, width = 17,height=11, dpi = 300)
ydata2 <- ydatar[Sind2, ]
ind1 <- which(is.na(rowMeans(ydata2)==T))
if(length(ind1)) {ydata2 <- ydata2[-ind1, ]}
day2 = apply(ydata2,2,median)
#day2 = colMeans(ydata2)
plot(day2,type="l")
ydata3 <- ydatar[Sind3, ]
ind1 <- which(is.na(rowMeans(ydata3)==T))
if(length(ind1)) {ydata3 <- ydata1[-ind1, ]}
day3 = apply(ydata3,2,median)
#day3 = colMeans(ydata3)
plot(day3,type="l")
ydata4 <- ydatar[Sind4, ]
ind1 <- which(is.na(rowMeans(ydata4)==T))
if(length(ind1)) {ydata4 <- ydata4[-ind1, ]}
day4 = apply(ydata4,2,median)
#day4 = colMeans(ydata4)
plot(day4,type="l")
ydata5 <- ydatar[Sind5, ]
ind1 <- which(is.na(rowMeans(ydata5)==T))
if(length(ind1)) {ydata5 <- ydata5[-ind1, ]}
day5 = apply(ydata5,2,median)
#day5 = colMeans(ydata5)
plot(day5,type="l")
ydata6 <- ydatar[Sind6, ]
ind1 <- which(is.na(rowMeans(ydata6)==T))
if(length(ind1)) {ydata6 <- ydata6[-ind1, ]}
day6 = apply(ydata6,2,median)
#day6 = colMeans(ydata6)
plot(day6,type="l")
ydata7 <- ydatar[Sind7, ]
ind1 <- which(is.na(rowMeans(ydata7)==T))
if(length(ind1)) {ydata7 <- ydata7[-ind1, ]}
day7 = apply(ydata7,2,median)
#day7 = colMeans(ydata7)
plot(day7,type="l")
par(mfrow=c(2,4))
plot(day1,type="l")
plot(day2,type="l")
plot(day3,type="l")
plot(day4,type="l")
plot(day5,type="l")
plot(day6,type="l")
plot(day7,type="l")
day1.med = data.frame(x=1:24,y=day1)
ggplot(data=day1.med,aes(x=x,y=y))+geom_point()+
geom_smooth(method=loess, color="blue")
day2.med = data.frame(x=1:24,y=day2)
ggplot(data=day2.med,aes(x=x,y=y))+geom_point()+
geom_smooth(method=loess, color="blue")
day3.med = data.frame(x=1:24,y=day3)
ggplot(data=day3.med,aes(x=x,y=y))+geom_point()+
geom_smooth(method=loess, color="blue")
day11 = data.frame(x=1:24,y=day1)
ggplot(data=day11,aes(x=x,y=y))+geom_point()+
geom_smooth(method=loess, color="blue")
library(rnhanesdata)
require(stats)
require(graphics)
library(splines2)
library(coneproj)
library(rcdd)
library(truncnorm)
library(truncdist)
library(mvtnorm)
library(R.utils)
library(Matrix)
library(igraph)
library(MCMCpack)
library(fda)
library(nnls)
###############model fitting
source("/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava_v1/fittingCompoSpikeFinal - Mixed effect Data.R")
source('/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava_v1/fittingCompo - Mixed effect Data.R')
Ti=24
t=1:24
i1 = 5
i2 = 8
i3 = 20
A = matrix(0,nrow=Ti+2,ncol=Ti)
for(j in 1:i1){
A[j,j] = 1.0
}
A[(i1+1),i1] = -1.0
A[(i1+1),(i1+1)] = 1.0
for(j in i1:(i2-1)){
A[j+2,j] = t[(j+2)]-t[(j+1)]
A[j+2,(j+1)] = t[j]-t[(j+2)]
A[j+2,(j+2)] = t[j+1]-t[j]
}
for(j in (i3):(Ti-2)){
A[j+2,j] = t[(j+2)]-t[(j+1)]
A[j+2,(j+1)] = t[j]-t[(j+2)]
A[j+2,(j+2)] = t[j+1]-t[j]
}
for(j in (i2):(i3-1)){
A[j+2,j] = t[(j+1)]-t[(j+2)]
A[j+2,(j+1)] = -t[j]+t[(j+2)]
A[j+2,(j+2)] = -t[j+1]+t[j]
}
## added by me
#A[i2-1,(i2-2):i2] = - A[i2-1,(i2-2):i2]
A[(Ti+1),(Ti-1)] = 1.0
A[(Ti+1),Ti] = -1.0
A[(Ti+2),Ti] = 1.0
m = nrow(A)
### makeH uses Ax <= 0; so multiply by -1
B = -A
qux <- makeH(B, rep(0, m))
### makeH produces output A such that Ax >= 0
#print(qux)
out <- scdd(d2q(qux),adjacency = T,incidence = T,inputincidence = T,inputadjacency = T, representation = "H")
out1 = q2d(out$output)
Delta = t(out1[,-c(1:2)])
dim(Delta)
adjacency = out$adjacency
adj.list = graph_from_adj_list(adjacency, mode = c("out", "in", "all", "total"),duplicate = TRUE)
adj.mat = as_adjacency_matrix(adj.list, sparse=F)
### cliques
graph.list = graph_from_adjacency_matrix(adj.mat, "undirected", weighted=T, diag=F)
clique.list = max_cliques(graph.list)
clique.size = unlist(lapply(clique.list,length))
degree = clique.size
l = count_max_cliques(graph.list)
J    <- 24
knot <- J-3
gamma_ls <- list()
Ti=24
#n = nrow(ydata3.scale)
n = nrow(ydata1)
xt = rep(1:Ti,n)
y = as.numeric(t(ydata1))
#y = array(ydata3.scale)
BS <- matrix(0, length(y), Ti)
BS[cbind(1:length(y), xt)] <- 1
Deltatilde <- BS %*% Delta
Als <- list()
for(i in 1:n){
ni <- 1:Ti
Als[[i]] <- bsplineS(xt[ni]/Ti, breaks = seq(0, 1, 1/knot))
}
X <- Deltatilde
cliques=clique.list
Total_itr=30
id = rep(1:n,each=Ti)
### ma and wo ma
out1 <- fitting(y, X, diag(Ti), cliques=clique.list, Total_itr=100, id, Als)
out2 <- fittingUN(y, xt, Ti, Total_itr=100)
out3 <- fittingUN(y, xt, Ti, splin=F, Total_itr=100)
library(rnhanesdata)
library(ggplot2)
## load the data
data("PAXINTEN_C");data("PAXINTEN_D")
data("Flags_C");data("Flags_D")
#data("Mortality_2011_C");data("Mortality_2011_D")
data("Covariate_C");data("Covariate_D")
data("Mortality_2015_C");data("Mortality_2015_D")
## 60*24=1440 minute level data to 24 hour data
timevec <- function(x){
mat <- matrix(x, 60, 24)
return(array(colMeans(mat, na.rm = T)))
}
y <- as.matrix(PAXINTEN_C[, 6:ncol(PAXINTEN_C)])
ydata <- apply(y, 1, timevec)
ydata <- t(ydata)
### 7176 participants for 7 days = 50232
subfreq <- array(table(PAXINTEN_C$SEQN))
sub  <- unique(PAXINTEN_C$SEQN)
### covariates for defining inclusion criteria
indr <- which(Covariate_C$SEQN %in% PAXINTEN_C$SEQN)
Covariate_Cr <- Covariate_C[indr, ]
#Inclusion criteria
ind1 <- which(as.numeric(Covariate_Cr$RIDAGEYR) < 40 &
as.numeric(Covariate_Cr$RIDAGEYR) > 30 &
Covariate_Cr$EducationAdult == "College graduate or above"
)
length(ind1)
samsub <- Covariate_Cr$SEQN[ind1]
### back to activity data
ind <- which(PAXINTEN_C$SEQN %in% samsub)
ydatar <- ydata[ind, ]
Sind1 <- (0:(nrow(ydatar)/7-1))*7+1
Sind2 <- (0:(nrow(ydatar)/7-1))*7+2
Sind3 <- (0:(nrow(ydatar)/7-1))*7+3
Sind4 <- (0:(nrow(ydatar)/7-1))*7+4
Sind5 <- (0:(nrow(ydatar)/7-1))*7+5
Sind6 <- (0:(nrow(ydatar)/7-1))*7+6
Sind7 <- (0:(nrow(ydatar)/7-1))*7+7
ydata1 <- ydatar[Sind1, ]
ind1 <- which(is.na(rowMeans(ydata1)==T))
if(length(ind1)) {ydata1 <- ydata1[-ind1, ]}
day1 = apply(ydata1,2,median)
day1.mean = colMeans(ydata1)
plot(day1.mean,type="l")
plot(day1,type="l")
library(rnhanesdata)
require(stats)
require(graphics)
library(splines2)
library(coneproj)
library(rcdd)
library(truncnorm)
library(truncdist)
library(mvtnorm)
library(R.utils)
library(Matrix)
library(igraph)
library(MCMCpack)
library(fda)
library(nnls)
###############model fitting
source("/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava_v1/fittingCompoSpikeFinal - Mixed effect Data.R")
source('/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava_v1/fittingCompo - Mixed effect Data.R')
Ti=24
t=1:24
i1 = 5
i2 = 8
i3 = 20
A = matrix(0,nrow=Ti+2,ncol=Ti)
for(j in 1:i1){
A[j,j] = 1.0
}
A[(i1+1),i1] = -1.0
A[(i1+1),(i1+1)] = 1.0
for(j in i1:(i2-1)){
A[j+2,j] = t[(j+2)]-t[(j+1)]
A[j+2,(j+1)] = t[j]-t[(j+2)]
A[j+2,(j+2)] = t[j+1]-t[j]
}
for(j in (i3):(Ti-2)){
A[j+2,j] = t[(j+2)]-t[(j+1)]
A[j+2,(j+1)] = t[j]-t[(j+2)]
A[j+2,(j+2)] = t[j+1]-t[j]
}
for(j in (i2):(i3-1)){
A[j+2,j] = t[(j+1)]-t[(j+2)]
A[j+2,(j+1)] = -t[j]+t[(j+2)]
A[j+2,(j+2)] = -t[j+1]+t[j]
}
## added by me
#A[i2-1,(i2-2):i2] = - A[i2-1,(i2-2):i2]
A[(Ti+1),(Ti-1)] = 1.0
A[(Ti+1),Ti] = -1.0
A[(Ti+2),Ti] = 1.0
m = nrow(A)
### makeH uses Ax <= 0; so multiply by -1
B = -A
qux <- makeH(B, rep(0, m))
### makeH produces output A such that Ax >= 0
#print(qux)
out <- scdd(d2q(qux),adjacency = T,incidence = T,inputincidence = T,inputadjacency = T, representation = "H")
out1 = q2d(out$output)
Delta = t(out1[,-c(1:2)])
dim(Delta)
adjacency = out$adjacency
adj.list = graph_from_adj_list(adjacency, mode = c("out", "in", "all", "total"),duplicate = TRUE)
adj.mat = as_adjacency_matrix(adj.list, sparse=F)
### cliques
graph.list = graph_from_adjacency_matrix(adj.mat, "undirected", weighted=T, diag=F)
clique.list = max_cliques(graph.list)
clique.size = unlist(lapply(clique.list,length))
degree = clique.size
l = count_max_cliques(graph.list)
J    <- 24
knot <- J-3
gamma_ls <- list()
Ti=24
#n = nrow(ydata3.scale)
n = nrow(ydata1)
xt = rep(1:Ti,n)
y = as.numeric(t(ydata1))
#y = array(ydata3.scale)
BS <- matrix(0, length(y), Ti)
BS[cbind(1:length(y), xt)] <- 1
Deltatilde <- BS %*% Delta
Als <- list()
for(i in 1:n){
ni <- 1:Ti
Als[[i]] <- bsplineS(xt[ni]/Ti, breaks = seq(0, 1, 1/knot))
}
X <- Deltatilde
cliques=clique.list
Total_itr=30
id = rep(1:n,each=Ti)
### ma and wo ma
out1 <- fitting(y, X, diag(Ti), cliques=clique.list, Total_itr=100, id, Als)
out2 <- fittingUN(y, xt, Ti, Total_itr=100)
out3 <- fittingUN(y, xt, Ti, splin=F, Total_itr=100)
result = data.frame(x=1:Ti,R = out1$esti,URS = out2$esti,UR = out3$esti)
library(dplyr)
library(reshape2)
library(ggplot2)
dat.true = data.frame(x=1:Ti,y=colMeans(ydata1))
df.plot = melt(result,id.vars = "x")
p1 = ggplot()+
geom_line(data = df.plot,aes(x=x,y=value,color=variable))+
#  scale_linetype_manual(values=c("dashed","solid","solid","solid"))+
theme(text = element_text(size = 30))+
geom_point(data = dat.true,aes(x=x,y=y))+ ylab("activity") + xlab("Time (in hours)")
p1
dat.true = data.frame(x=1:Ti,y=apply(ydata1,2,median))
df.plot = melt(result,id.vars = "x")
p1 = ggplot()+
geom_line(data = df.plot,aes(x=x,y=value,color=variable))+
#  scale_linetype_manual(values=c("dashed","solid","solid","solid"))+
theme(text = element_text(size = 30))+
geom_point(data = dat.true,aes(x=x,y=y))+ ylab("activity") + xlab("Time (in hours)")
p1
apply(ydata1,2,median)
dat.true = data.frame(x=1:Ti,y=colMeans(ydata1))
df.plot = melt(result,id.vars = "x")
p1 = ggplot()+
geom_line(data = df.plot,aes(x=x,y=value,color=variable))+
#  scale_linetype_manual(values=c("dashed","solid","solid","solid"))+
theme(text = element_text(size = 30))+
geom_point(data = dat.true,aes(x=x,y=y))+ ylab("activity") + xlab("Time (in hours)")
p1
View(A)
library(rnhanesdata)
require(stats)
require(graphics)
library(splines2)
library(coneproj)
library(rcdd)
library(truncnorm)
library(truncdist)
library(mvtnorm)
library(R.utils)
library(Matrix)
library(igraph)
library(MCMCpack)
library(fda)
library(nnls)
###############model fitting
source("/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava_v1/fittingCompoSpikeFinal - Mixed effect Data.R")
source('/Users/pro/Desktop/Backup/priors_on_cone/projects/arkaprava_v1/fittingCompo - Mixed effect Data.R')
Ti=24
t=1:24
i1 = 5
i2 = 8
i3 = 20
A = matrix(0,nrow=Ti+2,ncol=Ti)
for(j in 1:i1){
A[j,j] = 1.0
}
A[(i1+1),i1] = -1.0
A[(i1+1),(i1+1)] = 1.0
for(j in i1:(i2-1)){
A[j+2,j] = t[(j+2)]-t[(j+1)]
A[j+2,(j+1)] = t[j]-t[(j+2)]
A[j+2,(j+2)] = t[j+1]-t[j]
}
for(j in (i3):(Ti-2)){
A[j+2,j] = t[(j+2)]-t[(j+1)]
A[j+2,(j+1)] = t[j]-t[(j+2)]
A[j+2,(j+2)] = t[j+1]-t[j]
}
for(j in (i2):(i3-1)){
A[j+2,j] = t[(j+1)]-t[(j+2)]
A[j+2,(j+1)] = -t[j]+t[(j+2)]
A[j+2,(j+2)] = -t[j+1]+t[j]
}
## added by me
#A[i2-1,(i2-2):i2] = - A[i2-1,(i2-2):i2]
A[(Ti+1),(Ti-1)] = 1.0
A[(Ti+1),Ti] = -1.0
A[(Ti+2),Ti] = 1.0
J    <- 24
knot <- J-3
BS1 <- bsplineS((1:Ti)/Ti, breaks = seq(0, 1, 1/knot))
A <- A %*% BS1
m = nrow(A)
### makeH uses Ax <= 0; so multiply by -1
B = -A
qux <- makeH(B, rep(0, m))
### makeH produces output A such that Ax >= 0
#print(qux)
out <- scdd(d2q(qux),adjacency = T,incidence = T,inputincidence = T,inputadjacency = T, representation = "H")
out1 = q2d(out$output)
#Why removing first two columns? Is it specific to this simulation setting? Or should it be for any A?
Delta = t(out1[,-c(1:2)])
dim(Delta)
adjacency = out$adjacency
adj.list = graph_from_adj_list(adjacency, mode = c("out", "in", "all", "total"),duplicate = TRUE)
adj.mat = as_adjacency_matrix(adj.list, sparse=F)
### cliques
graph.list = graph_from_adjacency_matrix(adj.mat, "undirected", weighted=T, diag=F)
clique.list = max_cliques(graph.list)
clique.size = unlist(lapply(clique.list,length))
degree = clique.size
l = count_max_cliques(graph.list)
gamma_ls <- list()
Ti=24
#n = nrow(ydata3.scale)
n = nrow(ydata1)
xt = rep(1:Ti,n)
y = as.numeric(t(ydata1))
#y = array(ydata3.scale)
BS <- bsplineS(xt/Ti, breaks = seq(0, 1, 1/knot))
Deltatilde <- BS %*% Delta
Als <- list()
for(i in 1:n){
#i=1
ni <- 1:Ti
Als[[i]] <- bsplineS(xt[ni]/Ti, breaks = seq(0, 1, 1/knot))
}
X <- Deltatilde
cliques=clique.list
Total_itr=30
id = rep(1:n,each=Ti)
out4 <- fitting(y, X, BS1, cliques=clique.list, Total_itr=100, id, Als)
result1 = data.frame(x=1:Ti,R = out1$esti,URS = out2$esti,UR = out3$esti)
View(result)
result = cbind(result,out4$esti)
colnames(result)[5] = "RS"
write.csv(result , "result_day1_median.csv")
setwd("~/Desktop/Backup/priors_on_cone/projects/arkaprava_v1/NHANES_analysis/not_log_transformed_new_constraints")
write.csv(result , "result_day1_median.csv")
library(dplyr)
library(reshape2)
library(ggplot2)
dat.true = data.frame(x=1:Ti,y=colMeans(ydata1))
df.plot = melt(result,id.vars = "x")
p1 = ggplot()+
geom_line(data = df.plot,aes(x=x,y=value,color=variable))+
#  scale_linetype_manual(values=c("dashed","solid","solid","solid"))+
theme(text = element_text(size = 30))+
geom_point(data = dat.true,aes(x=x,y=y))+ ylab("activity") + xlab("Time (in hours)")
p1
par(mfrow=c(1,1))
plot(out1$gamma)
par(mfrow=c(2,1))
plot(out1$esti,type="l",ylim=c(0,355))
out1
